{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3489705-2492-4e99-95d5-be2a9c46da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal imports \n",
    "from MIL import build_model \n",
    "from utils.generic_utils import print_network \n",
    "\n",
    "#external imports \n",
    "import os \n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2209299-2da1-44a0-aa85-62e635eabee5",
   "metadata": {},
   "source": [
    "# Load Best-performing model for Masses (FPN-AbMIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88769fba-44f7-41a6-b725-6b18cd4f8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ViNDr'\n",
    "        self.label = 'Mass'\n",
    "        self.n_class = 1\n",
    "        self.train = False\n",
    "        # Data settings\n",
    "        self.img_size = [1520, 912]\n",
    "        self.patch_size = 512 \n",
    "        # Mammo-CLIP settings\n",
    "        self.clip_chk_pt_path = 'Mammo-CLIP_Checkpoints/b2-model-best-epoch-10.tar'\n",
    "        self.arch = 'upmc_breast_clip_det_b5_period_n_ft'\n",
    "        self.model_type =\"Classifier\"\n",
    "        self.feat_dim = 352\n",
    "        self.feature_extraction = 'offline' # uses pre-trained features from Mammo-clip. If desired to use input images, set to 'online'\n",
    "        # FPN-MIL model parameters\n",
    "        self.mil_type = 'pyramidal_mil'\n",
    "        self.nested_model = False\n",
    "        self.multi_scale_model = 'fpn'\n",
    "        self.fpn_dim = 256 \n",
    "        self.upsample_method = 'nearest'\n",
    "        self.norm_fpn = False\n",
    "        self.drop_classhead = 0.0\n",
    "        self.map_prob_func = 'softmax'\n",
    "        self.type_mil_encoder = 'mlp'\n",
    "        self.fcl_encoder_dim = 256 \n",
    "        self.fcl_dropout = 0.25\n",
    "        self.pooling_type = 'gated-attention'\n",
    "        self.deep_supervision = True \n",
    "        self.type_scale_aggregator = 'gated-attention'\n",
    "        self.fcl_attention_dim = 128 \n",
    "        self.drop_attention_pool = 0.25\n",
    "        self.scales = [16, 32, 128]\n",
    "        \n",
    "# Create an instance of the Args class\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9da44c8f-207a-4a29-ac0b-533bba115788",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_masses = build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "384654b3-768f-4b95-93fb-9fc1f5f3da24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyramidalMILmodel(\n",
      "  (inst_encoder): FeaturePyramidNetwork(\n",
      "    (inner_blocks): ModuleDict(\n",
      "      (inner_block_0): Sequential(\n",
      "        (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (inner_block_1): Sequential(\n",
      "        (0): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (layer_blocks): ModuleDict(\n",
      "      (layer_block_0): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (layer_block_1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (side_inst_aggregator): ModuleDict(\n",
      "    (encoders): ModuleDict(\n",
      "      (encoder_16): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_32): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_128): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (aggregators): ModuleDict(\n",
      "      (aggregator_16): Gated_Attn_Net(\n",
      "        (attention_V): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_U): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_weights): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (aggregator_32): Gated_Attn_Net(\n",
      "        (attention_V): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_U): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_weights): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "      (aggregator_128): Gated_Attn_Net(\n",
      "        (attention_V): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_U): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): Dropout(p=0.25, inplace=False)\n",
      "        )\n",
      "        (attention_weights): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (side_classifiers): ModuleDict(\n",
      "    (classifier_16): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (classifier_32): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (classifier_128): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (scale_aggregator): Gated_Attn_Net(\n",
      "    (attention_V): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_U): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_weights): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (classifier): head(\n",
      "    (drop): Dropout(p=0.0, inplace=True)\n",
      "    (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total number of parameters: 1.763592 M\n",
      "Total number of trainable parameters: 1.763592 M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load('FPN-MIL_Checkpoints/best_FPN-MIL_mass.pth', map_location='cpu')\n",
    "best_model_masses.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "best_model_masses = best_model_masses.to(device)\n",
    "print_network(best_model_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75e9d5-0925-4f18-b7bf-dbf1435258a4",
   "metadata": {},
   "source": [
    "# Load Best-performing model for Calcifications (FPN-SetTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9e1dc1e-44a5-482a-905c-f43a77b824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ViNDr'\n",
    "        self.label = 'Suspicious_Calcification'\n",
    "        self.n_class = 1\n",
    "        self.train = False\n",
    "        # Data settings\n",
    "        self.img_size = [1520, 912]\n",
    "        self.patch_size = 512 \n",
    "        # Mammo-CLIP settings\n",
    "        self.clip_chk_pt_path = 'Mammo-CLIP_Checkpoints/b2-model-best-epoch-10.tar'\n",
    "        self.arch = 'upmc_breast_clip_det_b5_period_n_ft'\n",
    "        self.model_type =\"Classifier\"\n",
    "        self.feat_dim = 352\n",
    "        self.feature_extraction = 'offline' # uses pre-trained features from Mammo-clip. If desired to use input images, set to 'online'\n",
    "        # FPN-MIL model parameters\n",
    "        self.mil_type = 'pyramidal_mil'\n",
    "        self.nested_model = False\n",
    "        self.multi_scale_model = 'fpn'\n",
    "        self.fpn_dim = 256 \n",
    "        self.upsample_method = 'nearest'\n",
    "        self.norm_fpn = False\n",
    "        self.drop_classhead = 0.0\n",
    "        self.map_prob_func = 'softmax'\n",
    "        self.type_mil_encoder = 'isab'\n",
    "        self.fcl_encoder_dim = 256 \n",
    "        self.isab_num_heads = 4\n",
    "        self.num_encoder_blocks = 2 \n",
    "        self.pooling_type = 'pma'\n",
    "        self.pma_num_heads = 1\n",
    "        self.drop_mha = 0.0\n",
    "        self.trans_layer_norm = True\n",
    "        self.deep_supervision = True \n",
    "        self.type_scale_aggregator = 'gated-attention'\n",
    "        self.fcl_attention_dim = 128 \n",
    "        self.drop_attention_pool = 0.25\n",
    "        self.scales = [16, 32, 128]\n",
    "        \n",
    "# Create an instance of the Args class\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b39ebf1c-c34d-486b-9e8b-1ca6266c2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_calcifications = build_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3348f3d4-50e2-45de-b9a2-e7a409907b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyramidalMILmodel(\n",
      "  (inst_encoder): FeaturePyramidNetwork(\n",
      "    (inner_blocks): ModuleDict(\n",
      "      (inner_block_0): Sequential(\n",
      "        (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (inner_block_1): Sequential(\n",
      "        (0): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (layer_blocks): ModuleDict(\n",
      "      (layer_block_0): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (layer_block_1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (side_inst_aggregator): ModuleDict(\n",
      "    (encoders): ModuleDict(\n",
      "      (encoder_16): ModuleList(\n",
      "        (0-1): 2 x InducedSetAttentionBlock(256, d_hidden = 256, num_induced_points=38, heads=4, layer_norm=True, activation=softmax)\n",
      "      )\n",
      "      (encoder_32): ModuleList(\n",
      "        (0-1): 2 x InducedSetAttentionBlock(256, d_hidden = 256, num_induced_points=32, heads=4, layer_norm=True, activation=softmax)\n",
      "      )\n",
      "      (encoder_128): ModuleList(\n",
      "        (0-1): 2 x InducedSetAttentionBlock(256, d_hidden = 256, num_induced_points=20, heads=4, layer_norm=True, activation=softmax)\n",
      "      )\n",
      "    )\n",
      "    (aggregators): ModuleDict(\n",
      "      (aggregator_16): PoolingByMultiheadAttention(256, num_seed_points=1, heads=1, layer_norm=True, activation=softmax)\n",
      "      (aggregator_32): PoolingByMultiheadAttention(256, num_seed_points=1, heads=1, layer_norm=True, activation=softmax)\n",
      "      (aggregator_128): PoolingByMultiheadAttention(256, num_seed_points=1, heads=1, layer_norm=True, activation=softmax)\n",
      "    )\n",
      "  )\n",
      "  (side_classifiers): ModuleDict(\n",
      "    (classifier_16): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (classifier_32): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (classifier_128): head(\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (scale_aggregator): Gated_Attn_Net(\n",
      "    (attention_V): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_U): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_weights): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (classifier): head(\n",
      "    (drop): Dropout(p=0.0, inplace=True)\n",
      "    (head_classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total number of parameters: 5.378181 M\n",
      "Total number of trainable parameters: 5.378181 M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load('FPN-MIL_Checkpoints/best_FPN-MIL_calcifications.pth', map_location='cpu')\n",
    "best_model_calcifications.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "best_model_calcifications = best_model_calcifications.to(device)\n",
    "print_network(best_model_calcifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb85ece-263b-4cfd-bb80-46d534ab749a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

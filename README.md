# Multi-scale Attention-based Multiple Instance Learning (MIL)

This repository contains the **official implementation** of the paper:

ğŸ“„ [**"Multi-scale Attention-based Multiple Instance Learning for Breast Cancer Diagnosis"**](https://link.springer.com/chapter/10.1007/978-3-032-05182-0_36)

Accepted at the **MICCAI 2025** conference for oral presentation and poster session. 

## Repository Structure

```plaintext
Multi-scale-Attention-based-MIL/
â”‚â”€â”€ Datasets/                  
â”‚   â”œâ”€â”€ dataset_concepts.py                 # Dataset classes for MIL classification & lesion detection   
â”‚   â””â”€â”€ dataset_utils.py                    # Data preprocessing & loading    
â”‚â”€â”€ Feature_Extractors/                   
â”‚   â”œâ”€â”€ __init__.py                         # Load feature extractor  
â”‚   â”œâ”€â”€ FPN.py                              # Feature Pyramid Network
â”‚   â””â”€â”€ mammoclip/                          
â”‚       â”œâ”€â”€ __init__.py                     # Load pre-trained MammoCLIP image encoder
â”‚       â”œâ”€â”€ efficient_net_custom_utils.py   # Helper functions for EfficientNet 
â”‚       â””â”€â”€ efficientnet_custom.py          # Modified EfficientNet implementation 
â”‚â”€â”€ MIL/                                    
â”‚   â”œâ”€â”€ __init__.py                         # Build & configure MIL model
â”‚   â”œâ”€â”€ MIL_models.py                       # MIL model architectures
â”‚   â”œâ”€â”€ AttentionModels.py                  # Attention-based MIL modules   
â”‚   â”œâ”€â”€ MIL_experiment.py                   # MIL classification training & evaluation 
â”‚   â”œâ”€â”€ inference_MIL_classifier.py         # MIL classification inference   
â”‚   â””â”€â”€ roi_eval.py                         # Lesion detection MIL evaluation 
â”‚â”€â”€ utils/                                  
â”‚   â”œâ”€â”€ data_split_utils.py                 # Data splitting utilities    
â”‚   â”œâ”€â”€ generic_utils.py                    # General-purpose utilities 
â”‚   â”œâ”€â”€ plot_utils.py                       # Plotting & visualization 
â”‚   â”œâ”€â”€ training_setup_utils.py             # Training setup & configuration
â”‚   â””â”€â”€ metrics.py                          # Evaluation metrics 
â”‚â”€â”€ vindrmammo_grouped_df.csv               # Dataset metadata (e.g. file names & labels)  
â”‚â”€â”€ main.py                                 # Main script to train & evaluate implemented models 
â”‚â”€â”€ offline_feature_extraction.py           # Script for offline feature extraction  
````

# Data Download

As mentioned in the paper, this work uses the preprocessed images provided by Ghosh et al. in their Mammo-CLIP work. 
- [Link to input images](https://www.kaggle.com/datasets/shantanughosh/vindr-mammogram-dataset-dicom-to-png)

Regarding metadata information for the downstream tasks (image classification and lesion detection), please use the **vindrmammo_grouped_df.csv** provided in this repository. 

# Feature Extraction 

Following prior deep MIL models that handle large-size bags, the implemented framework uses the pretrained EfficientNet-B2 image encoder from the Mammo-CLIP work as the backbone for feature extraction. 
- [Link for the pretrained EfficientNet-B2](https://huggingface.co/shawn24/Mammo-CLIP/blob/main/Pre-trained-checkpoints/b2-model-best-epoch-10.tar)

After successfully downloading the image encoder checkpoint, you need to set the --clip_chk_pt_path argument to the correct path. 

The implemented framework is compatible with both online and offline feature extraction. To perform offline feature extraction, run the following code:

```bash
python offline_feature_extraction.py \
  --clip_chk_pt_path "foundational_models/Mammo-CLIP-main/b2-model-best-epoch-10.tar" \ # Path to Mammo-CLIP's image encoder checkpoint
  --dataset 'ViNDr' \
  --arch 'upmc_breast_clip_det_b2_period_n_lp' \
  --csv-file 'vindrmammo_grouped_df.csv' \
  --feat_dir 'PreProcessedData/Vindir-mammoclip/extracted_features' \
  --patching \ # Wether to perform patching on full-resolution images. If false, it will consider previously extracted patches that were saved in a directory
  --patch_size 512 \ 
  --overlap 0.0 \
  --multi_scale_model 'fpn'
```

# Checkpoints

We provide pre-training checkpoints for our best-performing models.  

|Description         | Checkpoints |
|--------------------|-------------|
| Best model for calcifications | [FPN-SetTrans](https://drive.google.com/file/d/1pcr5wa8cI7R8L-7MfkXBEBB2IE02NmMI/view?usp=sharing) |
| Best model for masses | [FPN-AbMIL](https://drive.google.com/file/d/1ptgub09TjB2oCpm2ij2OyaVDKT_5y8D0/view?usp=sharing) |

## â³ Coming Soon (under construction) 
- [ ] Scripts for training and evaluating **MIL classification tasks**
- [ ] Scripts for post-hoc evaluation of **lesion detection tasks**
- [ ] Jupyter Notebooks to reproduce **MICCAI 2025 results**

## Reference 

If you find our work useful in your research or if you use parts of this code, please use the following BibTeX entry.

```plaintext
@InProceedings{MouMar_Multiscale_MICCAI2025,
        author = { MourÃ£o, Mariana AND Nascimento, Jacinto C. AND Santiago, Carlos AND Silveira, Margarida},
        title = { { Multi-scale Attention-based Multiple Instance Learning for Breast Cancer Diagnosis } },
        booktitle = {proceedings of Medical Image Computing and Computer Assisted Intervention -- MICCAI 2025},
        year = {2025},
        publisher = {Springer Nature Switzerland},
        volume = {LNCS 15974},
        month = {September},
        page = {364 -- 374}
}
````
